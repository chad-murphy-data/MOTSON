name: Evaluate Predictions

on:
  # Run after match results are fetched (triggered by fetch-results workflow)
  workflow_run:
    workflows: ["Fetch Match Results"]
    types:
      - completed

  # Run weekly on Tuesday (after Monday evaluations are complete)
  schedule:
    - cron: '0 10 * * 2'  # 10 AM UTC every Tuesday

  # Manual trigger
  workflow_dispatch:
    inputs:
      week:
        description: 'Evaluate up to this matchweek (leave empty for all)'
        required: false
        type: string
      include_opta:
        description: 'Attempt to fetch Opta predictions (may fail)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    # Only run if triggered manually OR if the fetch-results workflow succeeded
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      github.event.workflow_run.conclusion == 'success'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create cache directory
        run: mkdir -p data/evaluation_cache

      - name: Get current matchweek
        id: week
        run: |
          WEEK=$(python3 -c "
          import sqlite3
          conn = sqlite3.connect('data/motson.db')
          c = conn.cursor()
          c.execute('SELECT MAX(matchweek) FROM match_results')
          result = c.fetchone()[0]
          print(result if result else 0)
          ")
          echo "current=$WEEK" >> $GITHUB_OUTPUT
          echo "Current matchweek: $WEEK"

          # Use input week if provided, otherwise use current
          if [ -n "${{ github.event.inputs.week }}" ]; then
            echo "target=${{ github.event.inputs.week }}" >> $GITHUB_OUTPUT
          else
            echo "target=$WEEK" >> $GITHUB_OUTPUT
          fi

      - name: Check for betting odds data
        id: odds_check
        run: |
          if [ -f "data/evaluation_cache/betting_odds.csv" ]; then
            ROW_COUNT=$(tail -n +2 "data/evaluation_cache/betting_odds.csv" | wc -l)
            echo "Betting odds file found with $ROW_COUNT matches"
            echo "has_odds=true" >> $GITHUB_OUTPUT
            echo "odds_count=$ROW_COUNT" >> $GITHUB_OUTPUT
          else
            echo "No betting odds file found"
            echo "has_odds=false" >> $GITHUB_OUTPUT
            echo "odds_count=0" >> $GITHUB_OUTPUT
          fi

      - name: Fetch betting odds if missing
        if: steps.odds_check.outputs.has_odds == 'false'
        run: |
          echo "Attempting to fetch betting odds..."
          # Determine season
          MONTH=$(date +%m)
          YEAR=$(date +%Y)
          if [ "$MONTH" -lt 7 ]; then
            SEASON=$((YEAR - 1))
          else
            SEASON=$YEAR
          fi
          SEASON_CODE="${SEASON: -2}$((${SEASON: -2} + 1))"

          URL="https://www.football-data.co.uk/mmz4281/${SEASON_CODE}/E0.csv"
          OUTPUT="data/evaluation_cache/betting_odds.csv"

          curl -sSL -o "$OUTPUT" "$URL" || echo "Failed to fetch betting odds"

          if [ -f "$OUTPUT" ]; then
            ROW_COUNT=$(tail -n +2 "$OUTPUT" | wc -l)
            echo "Downloaded $ROW_COUNT matches"
          fi

      - name: Fetch Opta predictions
        id: opta_fetch
        continue-on-error: true
        if: github.event.inputs.include_opta == 'true' || github.event_name == 'schedule'
        run: |
          echo "Attempting to fetch Opta predictions from The Analyst..."

          # The Analyst predictions page
          OPTA_URL="https://theanalyst.com/articles/premier-league-match-predictions"
          RAW_FILE="data/evaluation_cache/opta_raw.html"
          OUTPUT_FILE="data/evaluation_cache/opta_predictions.json"

          # Fetch the page with retries
          MAX_RETRIES=3
          RETRY_COUNT=0
          SUCCESS=false

          while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$SUCCESS" = "false" ]; do
            HTTP_CODE=$(curl -sSL -o "$RAW_FILE" -w "%{http_code}" \
              -H "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36" \
              -H "Accept: text/html,application/xhtml+xml" \
              "$OPTA_URL" 2>/dev/null)

            if [ "$HTTP_CODE" = "200" ] && [ -s "$RAW_FILE" ]; then
              SUCCESS=true
              echo "Successfully fetched Opta predictions page"
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "Attempt $RETRY_COUNT failed (HTTP $HTTP_CODE), retrying..."
              sleep 3
            fi
          done

          if [ "$SUCCESS" = "false" ]; then
            echo "Failed to fetch Opta predictions after $MAX_RETRIES attempts"
            echo "opta_success=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Parse the HTML to extract predictions
          python3 << 'PYEOF'
          import json
          import re
          from pathlib import Path

          raw_file = Path("data/evaluation_cache/opta_raw.html")
          output_file = Path("data/evaluation_cache/opta_predictions.json")

          if not raw_file.exists():
              print("No raw HTML file found")
              exit(0)

          html = raw_file.read_text(errors='replace')

          # Team name normalization
          TEAM_MAP = {
              "Man United": "Manchester Utd",
              "Man City": "Manchester City",
              "Newcastle": "Newcastle Utd",
              "Nott'm Forest": "Nott'ham Forest",
              "Nottingham Forest": "Nott'ham Forest",
              "Spurs": "Tottenham",
              "Manchester United": "Manchester Utd",
              "Newcastle United": "Newcastle Utd",
              "Tottenham Hotspur": "Tottenham",
              "West Ham United": "West Ham",
              "Brighton & Hove Albion": "Brighton",
              "AFC Bournemouth": "Bournemouth",
              "Wolverhampton Wanderers": "Wolves",
          }

          def normalize(name):
              name = name.strip()
              return TEAM_MAP.get(name, name)

          predictions = {}

          # Try multiple patterns to extract predictions
          # Pattern 1: JSON embedded in the page
          json_pattern = r'"predictions?":\s*\[(.*?)\]'
          matches = re.findall(json_pattern, html, re.DOTALL)

          # Pattern 2: Table rows with percentages
          table_pattern = r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:vs?\.?|v)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)[^0-9]*?(\d+\.?\d*)\s*%?\s*[-â€“/|]\s*(\d+\.?\d*)\s*%?\s*[-â€“/|]\s*(\d+\.?\d*)\s*%?'
          table_matches = re.findall(table_pattern, html)

          for match in table_matches:
              try:
                  home_team = normalize(match[0])
                  away_team = normalize(match[1])
                  home_prob = float(match[2])
                  draw_prob = float(match[3])
                  away_prob = float(match[4])

                  # Convert percentages if needed
                  if home_prob > 1 or draw_prob > 1 or away_prob > 1:
                      home_prob /= 100
                      draw_prob /= 100
                      away_prob /= 100

                  total = home_prob + draw_prob + away_prob
                  if total < 0.9 or total > 1.1:
                      continue

                  # Normalize
                  home_prob /= total
                  draw_prob /= total
                  away_prob /= total

                  key = f"{home_team}|{away_team}"
                  predictions[key] = {
                      "home_team": home_team,
                      "away_team": away_team,
                      "home_win_prob": home_prob,
                      "draw_prob": draw_prob,
                      "away_win_prob": away_prob,
                      "source": "opta",
                      "matchweek": 0,
                      "match_id": ""
                  }
              except (ValueError, IndexError):
                  continue

          if predictions:
              with open(output_file, 'w') as f:
                  json.dump(predictions, f, indent=2)
              print(f"Extracted {len(predictions)} Opta predictions")
          else:
              print("No predictions could be extracted from the page")
          PYEOF

          # Check if we got predictions
          if [ -f "$OUTPUT_FILE" ]; then
            PRED_COUNT=$(python3 -c "import json; print(len(json.load(open('$OUTPUT_FILE'))))" 2>/dev/null || echo "0")
            if [ "$PRED_COUNT" -gt "0" ]; then
              echo "opta_success=true" >> $GITHUB_OUTPUT
              echo "opta_count=$PRED_COUNT" >> $GITHUB_OUTPUT
            else
              echo "opta_success=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "opta_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Run evaluation
        id: evaluate
        run: |
          ARGS=""

          # Add week argument if specified
          if [ -n "${{ steps.week.outputs.target }}" ]; then
            ARGS="$ARGS --week ${{ steps.week.outputs.target }}"
          fi

          # Add betting CSV if available
          if [ -f "data/evaluation_cache/betting_odds.csv" ]; then
            ARGS="$ARGS --betting-csv data/evaluation_cache/betting_odds.csv"
          fi

          # Add Opta JSON if available
          if [ -f "data/evaluation_cache/opta_predictions.json" ]; then
            ARGS="$ARGS --opta-json data/evaluation_cache/opta_predictions.json"
          fi

          # Output file
          OUTPUT_FILE="data/evaluation_cache/evaluation_results.json"
          ARGS="$ARGS --output $OUTPUT_FILE"

          echo "Running: python scripts/evaluate_predictions.py $ARGS"
          python scripts/evaluate_predictions.py $ARGS | tee evaluation_output.txt

          # Parse results for summary
          if [ -f "$OUTPUT_FILE" ]; then
            echo "results_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT

            # Extract key metrics
            BRIER=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(f\"{d['motson']['brier_score']:.4f}\")")
            LOGLOSS=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(f\"{d['motson']['log_loss']:.4f}\")")
            ACCURACY=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(f\"{d['motson']['accuracy']*100:.1f}\")")
            N_MATCHES=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(d['motson']['n_matches'])")

            echo "brier=$BRIER" >> $GITHUB_OUTPUT
            echo "logloss=$LOGLOSS" >> $GITHUB_OUTPUT
            echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
            echo "n_matches=$N_MATCHES" >> $GITHUB_OUTPUT

            # Check if betting metrics exist
            if python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); exit(0 if 'betting' in d else 1)" 2>/dev/null; then
              BETTING_BRIER=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(f\"{d['betting']['brier_score']:.4f}\")")
              BETTING_ACCURACY=$(python3 -c "import json; d=json.load(open('$OUTPUT_FILE')); print(f\"{d['betting']['accuracy']*100:.1f}\")")
              echo "betting_brier=$BETTING_BRIER" >> $GITHUB_OUTPUT
              echo "betting_accuracy=$BETTING_ACCURACY" >> $GITHUB_OUTPUT
              echo "has_betting=true" >> $GITHUB_OUTPUT
            else
              echo "has_betting=false" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Generate evaluation badge data
        run: |
          # Create badge data for README
          python3 << 'EOF'
          import json
          from pathlib import Path

          results_file = Path("data/evaluation_cache/evaluation_results.json")
          if not results_file.exists():
              print("No results file found")
              exit(0)

          with open(results_file) as f:
              data = json.load(f)

          motson = data.get("motson", {})

          badge_data = {
              "schemaVersion": 1,
              "label": "Brier Score",
              "message": f"{motson.get('brier_score', 0):.3f}",
              "color": "blue"
          }

          # Color based on score (lower is better)
          brier = motson.get('brier_score', 1)
          if brier < 0.55:
              badge_data["color"] = "brightgreen"
          elif brier < 0.60:
              badge_data["color"] = "green"
          elif brier < 0.65:
              badge_data["color"] = "yellowgreen"
          elif brier < 0.70:
              badge_data["color"] = "yellow"
          else:
              badge_data["color"] = "orange"

          with open("data/evaluation_cache/brier_badge.json", "w") as f:
              json.dump(badge_data, f)

          # Accuracy badge
          accuracy_badge = {
              "schemaVersion": 1,
              "label": "Accuracy",
              "message": f"{motson.get('accuracy', 0)*100:.1f}%",
              "color": "blue"
          }

          acc = motson.get('accuracy', 0)
          if acc > 0.55:
              accuracy_badge["color"] = "brightgreen"
          elif acc > 0.50:
              accuracy_badge["color"] = "green"
          elif acc > 0.45:
              accuracy_badge["color"] = "yellowgreen"
          else:
              accuracy_badge["color"] = "yellow"

          with open("data/evaluation_cache/accuracy_badge.json", "w") as f:
              json.dump(accuracy_badge, f)

          print(f"Badge data generated: Brier={brier:.3f}, Accuracy={acc*100:.1f}%")
          EOF

      - name: Check for changes
        id: changes
        run: |
          git add data/evaluation_cache/
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push
        if: steps.changes.outputs.has_changes == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git commit -m "Update prediction evaluation (Week ${{ steps.week.outputs.target }})

          MOTSON Metrics:
          - Brier Score: ${{ steps.evaluate.outputs.brier }}
          - Log Loss: ${{ steps.evaluate.outputs.logloss }}
          - Accuracy: ${{ steps.evaluate.outputs.accuracy }}%
          - Matches evaluated: ${{ steps.evaluate.outputs.n_matches }}

          Co-Authored-By: github-actions[bot] <github-actions[bot]@users.noreply.github.com>"
          git push

      - name: Create summary
        run: |
          echo "## ðŸ“Š MOTSON Prediction Evaluation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Matchweek:** ${{ steps.week.outputs.target }}" >> $GITHUB_STEP_SUMMARY
          echo "**Matches evaluated:** ${{ steps.evaluate.outputs.n_matches }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### MOTSON Performance" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Brier Score | ${{ steps.evaluate.outputs.brier }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Log Loss | ${{ steps.evaluate.outputs.logloss }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Accuracy | ${{ steps.evaluate.outputs.accuracy }}% |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.evaluate.outputs.has_betting }}" == "true" ]; then
            echo "### Comparison with Betting Odds" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | MOTSON | Betting |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|--------|---------|" >> $GITHUB_STEP_SUMMARY
            echo "| Brier Score | ${{ steps.evaluate.outputs.brier }} | ${{ steps.evaluate.outputs.betting_brier }} |" >> $GITHUB_STEP_SUMMARY
            echo "| Accuracy | ${{ steps.evaluate.outputs.accuracy }}% | ${{ steps.evaluate.outputs.betting_accuracy }}% |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Determine winner
            python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          with open("data/evaluation_cache/evaluation_results.json") as f:
              data = json.load(f)
          motson_brier = data["motson"]["brier_score"]
          betting_brier = data.get("betting", {}).get("brier_score", 999)
          diff = motson_brier - betting_brier
          if diff < -0.01:
              print(f"âœ… **MOTSON outperforms betting markets** by {abs(diff):.4f} Brier Score")
          elif diff > 0.01:
              print(f"âš ï¸ **Betting markets outperform MOTSON** by {diff:.4f} Brier Score")
          else:
              print(f"ðŸŸ° **MOTSON and betting markets perform similarly** (diff: {abs(diff):.4f})")
          EOF
          fi

          # Add betting strategy analysis if available
          if python3 -c "import json; d=json.load(open('data/evaluation_cache/evaluation_results.json')); exit(0 if 'betting_strategies' in d else 1)" 2>/dev/null; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ’° Betting Strategy Analysis" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          with open("data/evaluation_cache/evaluation_results.json") as f:
              data = json.load(f)

          bs = data.get("betting_strategies", {})
          strategies = bs.get("strategies", {})
          best = bs.get("best_strategy", {})

          print("| Strategy | Bets | Win Rate | ROI | Final $ |")
          print("|----------|------|----------|-----|---------|")

          for name in ["value_5pct", "value_10pct", "kelly_25pct", "predicted_winner"]:
              if name in strategies:
                  s = strategies[name]
                  print(f"| {name} | {s['n_bets']} | {s['win_rate']*100:.1f}% | {s['roi']:+.1f}% | ${s['final_bankroll']:.2f} |")

          print("")
          if best.get("roi", 0) > 0:
              print(f"âœ… **Best strategy: {best['name']}** with {best['roi']:+.1f}% ROI")
          else:
              print(f"âš ï¸ No profitable strategy found (best: {best.get('name', 'N/A')} at {best.get('roi', 0):.1f}% ROI)")
          EOF
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Interpretation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Brier Score:** Lower is better (0 = perfect, 2 = worst)" >> $GITHUB_STEP_SUMMARY
          echo "- **Log Loss:** Lower is better (~1.1 is typical for EPL)" >> $GITHUB_STEP_SUMMARY
          echo "- **Accuracy:** Higher is better (~50% is typical)" >> $GITHUB_STEP_SUMMARY

      - name: Upload evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            data/evaluation_cache/evaluation_results.json
            data/evaluation_cache/brier_badge.json
            data/evaluation_cache/accuracy_badge.json
            evaluation_output.txt
          retention-days: 90
